{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando Dataset do professor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape train:  (36, 360)\n",
      "shape test:  (24, 360)\n",
      "shape valid:  (24, 360)\n"
     ]
    }
   ],
   "source": [
    "# testando amostras do professor\n",
    "df_train = pd.read_csv('dataset/har-example-mo436/train.csv')\n",
    "df_test = pd.read_csv('dataset/har-example-mo436/test.csv')\n",
    "df_val = pd.read_csv('dataset/har-example-mo436/validation.csv')\n",
    "\n",
    "# pegando só porcentagens das amostras\n",
    "percent = 0.6\n",
    "porcentagem = f\"{percent} dos dados de treino do professor - com backbone\"\n",
    "df_90, df_10 = train_test_split(df_train, test_size=percent, random_state=42) # dataframes de treino e validacao\n",
    "df_train = df_10\n",
    "\n",
    "X_train = df_train.drop(columns=[df_train.columns[0], 'gyro-end-time', 'level_0', 'accel-end-time', 'gyro-start-time', 'index', 'user', 'serial', 'accel-start-time', 'csv', 'timestamp diff', 'activity code', 'window', 'standard activity code'])\n",
    "X_test = df_test.drop(columns=[df_train.columns[0], 'gyro-end-time', 'level_0', 'accel-end-time', 'gyro-start-time', 'index', 'user', 'serial', 'accel-start-time', 'csv', 'timestamp diff', 'activity code', 'window', 'standard activity code'])\n",
    "X_val = df_val.drop(columns=[df_train.columns[0], 'gyro-end-time', 'level_0', 'accel-end-time', 'gyro-start-time', 'index', 'user', 'serial', 'accel-start-time', 'csv', 'timestamp diff', 'activity code', 'window', 'standard activity code'])\n",
    "\n",
    "y_train = df_train.pop('standard activity code')\n",
    "y_test = df_test.pop('standard activity code')\n",
    "y_val = df_val.pop('standard activity code')\n",
    "\n",
    "print(\"shape train: \", X_train.shape)\n",
    "print(\"shape test: \", X_test.shape)\n",
    "print(\"shape valid: \", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVLabeledDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "        if self.transform:\n",
    "            self.features = self.transform(self.features)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.features[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalVAE(nn.Module):\n",
    "    def __init__(self, device, input_dim, hidden_dim, latent_dim, num_layers=1):\n",
    "        super(TemporalVAE, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        self.hidden2mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.hidden2logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.latent2hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(hidden_dim, input_dim, num_layers, batch_first=True, bidirectional=False, dropout=0.2)\n",
    "        self.output_layer = nn.Linear(input_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std).to(self.device)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def encode(self, x):\n",
    "        residual = x\n",
    "        output, (h_n, _) = self.encoder_lstm(x)\n",
    "        output = output[:, :, :output.size(2)//2] + output[:, :, output.size(2)//2:]\n",
    "        h_n = output[:, -1, :]\n",
    "        # h_n = h_n[-1, :, :]\n",
    "        h_n = self.dropout(h_n)\n",
    "        mean = F.leaky_relu(self.hidden2mean(h_n), negative_slope=0.01)\n",
    "        logvar = F.leaky_relu(self.hidden2logvar(h_n), negative_slope=0.01)\n",
    "        return mean, logvar, residual\n",
    "    \n",
    "    def decode(self, z, seq_len, residual):\n",
    "        # hidden = self.latent2hidden(z).unsqueeze(0).repeat(seq_len, 1, 1).transpose(0, 1)\n",
    "        hidden = self.latent2hidden(z).unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        output, _ = self.decoder_lstm(hidden)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # output += residual\n",
    "\n",
    "        output = self.output_layer(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, logvar, residual = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        seq_len = x.size(1)\n",
    "        recon_x = self.decode(z, seq_len, residual)\n",
    "        return recon_x, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device not found.\n",
      "cuda\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cuda\n",
    "print(device)\n",
    "\n",
    "input_dim = 360#train_dataset.features.shape[1]\n",
    "hidden_dim = 128 # 400\n",
    "latent_dim = 64 # 20\n",
    "num_layers = 2\n",
    "epochs = 350\n",
    "\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporalVAE(\n",
      "  (encoder_lstm): LSTM(360, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (hidden2mean): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (hidden2logvar): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (latent2hidden): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (decoder_lstm): LSTM(128, 360, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (output_layer): Linear(in_features=360, out_features=360, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# carregando modelo\n",
    "model_path = \"model/vae.pth\"\n",
    "\n",
    "# Criar uma instância do modelo\n",
    "vae = TemporalVAE(device, input_dim, hidden_dim, latent_dim, num_layers).to(device)\n",
    "\n",
    "# Carregar o estado salvo\n",
    "state_dict = torch.load(model_path)\n",
    "\n",
    "# Carregar o estado no modelo\n",
    "vae.load_state_dict(state_dict)\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo representações latentes do VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter para tensores\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "X_train_tensor = X_train_tensor.unsqueeze(1) # [batch_size, sequence_length, input_dim]\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "X_test_tensor = X_test_tensor.unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "X_val_tensor = X_val_tensor.unsqueeze(1)\n",
    "\n",
    "vae.eval()\n",
    "\n",
    "# Extrair representações latentes\n",
    "with torch.no_grad():\n",
    "    mean_train, _, _ = vae.encode(X_train_tensor)\n",
    "    mean_test, _, _ = vae.encode(X_test_tensor)\n",
    "    mean_val, _, _ = vae.encode(X_val_tensor)\n",
    "\n",
    "# Converter para numpy\n",
    "X_train_latent = mean_train.cpu().numpy()\n",
    "X_test_latent = mean_test.cpu().numpy()\n",
    "X_val_latent = mean_val.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Validação: 37.50%\n",
      "Acurácia Teste: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Normalizar as representações latentes\n",
    "scaler = StandardScaler()\n",
    "X_train_latent = scaler.fit_transform(X_train_latent)\n",
    "X_test_latent = scaler.transform(X_test_latent)\n",
    "X_val_latent = scaler.transform(X_val_latent)\n",
    "\n",
    "# Treinar o Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_latent, y_train)\n",
    "\n",
    "# Predição e avaliação\n",
    "y_pred_val = rf_model.predict(X_val_latent)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "y_pred_test = rf_model.predict(X_test_latent)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'Acurácia Validação: {accuracy_val * 100:.2f}%')\n",
    "print(f'Acurácia Teste: {accuracy_test * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Validação: 37.50%\n",
      "Acurácia Teste: 45.83%\n"
     ]
    }
   ],
   "source": [
    "# Treinar o GradientBoostingClassifier\n",
    "rf_model = GradientBoostingClassifier(n_estimators=250, learning_rate=0.1, max_depth=40, random_state=42)\n",
    "rf_model.fit(X_train_latent, y_train)\n",
    "\n",
    "# Predição e avaliação\n",
    "y_pred_val = rf_model.predict(X_val_latent)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "y_pred_test = rf_model.predict(X_test_latent)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'Acurácia Validação: {accuracy_val * 100:.2f}%')\n",
    "print(f'Acurácia Teste: {accuracy_test * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia na Validação: 58.33%\n",
      "Acurácia no Teste: 25.00%\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(kernel='linear', random_state=42)  # Você pode experimentar diferentes kernels, como 'rbf' ou 'poly'\n",
    "svm_model.fit(X_train_latent, y_train)\n",
    "\n",
    "# Predição\n",
    "y_pred_val = svm_model.predict(X_val_latent)\n",
    "y_pred_test = svm_model.predict(X_test_latent)\n",
    "\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'Acurácia na Validação: {accuracy_val * 100:.2f}%')\n",
    "print(f'Acurácia no Teste: {accuracy_test * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
